{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tcobolukbasi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime,re,json,pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insert_query_from_df(df, dest_table):\n",
    "\n",
    "    insert = \"\"\"\n",
    "    INSERT INTO '{dest_table}' (\n",
    "        \"\"\".format(dest_table=dest_table)\n",
    "\n",
    "    columns_string = str(list(df.columns))[1:-1]\n",
    "    columns_string = re.sub(r' ', '\\n        ', columns_string)\n",
    "    columns_string = re.sub(r'\\'', '', columns_string)\n",
    "\n",
    "    values_string = ''\n",
    "\n",
    "    for row in df.itertuples(index=False,name=None):\n",
    "        values_string += re.sub(r'nan', 'null', str(row))\n",
    "        values_string += ',\\n'\n",
    "\n",
    "    return insert + columns_string + ')\\n     VALUES\\n' + values_string[:-2] + ';'\n",
    "\n",
    "\n",
    "def BaseBand_parser(df_all):\n",
    "    # Extract Parameters\n",
    "    TimeStamp = df_all.beginTime.dropna().values[0]\n",
    "    SubNetwork1 = df_all[\"dnPrefix\"].dropna().values[0].split(\",\")[0].split(\"=\")[1]\n",
    "    SubNetwork2 = df_all[\"dnPrefix\"].dropna().values[0].split(\",\")[1].split(\"=\")[1]\n",
    "    SubNetwork3 = df_all[\"dnPrefix\"].dropna().values[0].split(\",\")[2].split(\"=\")[1]\n",
    "    MeContext = df_all[\"dnPrefix\"].dropna().values[0].split(\",\")[3].split(\"=\")[1]\n",
    "\n",
    "    # Extract measInfoId-measObjLdn Table\n",
    "    df3 = df_all[[\"measInfoId\",\"jobId\",\"measObjLdn\"]].dropna(how=\"all\")\n",
    "    df3[\"jobId\"] = df3[\"jobId\"].fillna(method=\"ffill\")\n",
    "    df3[\"measInfoId\"] = df3[\"measInfoId\"].fillna(method=\"ffill\")\n",
    "    df3 = df3.dropna()\n",
    "    df3[\"measInfoId\"] = df3[\"measInfoId\"] + \",JobId=\" + df3[\"jobId\"]\n",
    "    df3 = df3.drop([\"jobId\"],axis=1)\n",
    "\n",
    "    # Meas Values\n",
    "    df = df_all[[\"measInfoId\",\"jobId\",\"measObjLdn\",\"{http://www.3gpp.org/ftp/specs/archive/32_series/32.435#measCollec}r\",\"p\"]]\n",
    "    df = df.dropna(axis=0,how=\"all\")\n",
    "    df.columns = [\"measInfoId\",\"jobId\",\"measObjLdn\",\"value\",\"p\"]\n",
    "    df.loc[df[\"measObjLdn\"].notna(), 'value'] = None\n",
    "    df[\"measInfoId\"] = df[\"measInfoId\"].fillna(method=\"ffill\")\n",
    "    df[\"jobId\"] = df[\"jobId\"].fillna(method=\"ffill\")\n",
    "    df[\"measObjLdn\"] = df[\"measObjLdn\"].fillna(method=\"ffill\")\n",
    "    df[\"measInfoId\"] = df[\"measInfoId\"] + \",JobId=\" + df[\"jobId\"]\n",
    "    df = df.drop(\"jobId\",axis=1)\n",
    "    df = df.merge(df3,on=[\"measInfoId\",\"measObjLdn\"],how=\"inner\")\n",
    "\n",
    "\n",
    "    # Define Meas Types\n",
    "    df2 = df_all[[\"measInfoId\",\"jobId\",\"{http://www.3gpp.org/ftp/specs/archive/32_series/32.435#measCollec}measType\",\"p\"]]\n",
    "    df2.columns = [\"measInfoId\",\"jobId\",\"measType\",\"p\"]\n",
    "    df2 = df2.dropna(axis=0,how=\"all\")\n",
    "    df2[\"measInfoId\"] = df2[\"measInfoId\"].fillna(method=\"ffill\")\n",
    "    df2[\"jobId\"] = df2[\"jobId\"].fillna(method=\"ffill\")\n",
    "    df2[\"measType\"] = df2[\"measType\"].fillna(method=\"ffill\")\n",
    "    df2[\"measInfoId\"] = df2[\"measInfoId\"] + \",JobId=\" + df2[\"jobId\"]\n",
    "    df2 = df2.drop(\"jobId\",axis=1)\n",
    "    df2 = df2.dropna()\n",
    "    df_meastypes = df2.drop_duplicates([\"measInfoId\",\"measType\"])\n",
    "\n",
    "    #Combine All\n",
    "    df4 = df.merge(df_meastypes,on=[\"measInfoId\",\"p\"],how=\"left\")\n",
    "    df4 = df4.drop_duplicates()\n",
    "    df4 = df4[df4.measType.notna()]\n",
    "\n",
    "    # Seperate all PmGroups\n",
    "    measInfoIds = df4.measInfoId.unique()\n",
    "\n",
    "    # Prepare all counters list\n",
    "    all_counters = []\n",
    "    for measInfoId in measInfoIds:\n",
    "        if \"USERDEF-All_Scanners\" in measInfoId[:1]:\n",
    "            df5 = df4[df4[\"measInfoId\"]==measInfoId].pivot(index=\"measObjLdn\",columns=\"measType\",values=\"value\")\n",
    "            df5 = df5.rename_axis(\"ID\", axis=1).reset_index()\n",
    "            alist = list(df5.columns)\n",
    "            alist.remove(\"measObjLdn\")\n",
    "            all_counters = all_counters + alist\n",
    "\n",
    "    #dtyps = {}\n",
    "    for measInfoId in measInfoIds:\n",
    "        #print(\"\\n\\n######### \",measInfoId, \" #########\\n\\n\")\n",
    "        df5 = df4[df4[\"measInfoId\"]==measInfoId].pivot(index=\"measObjLdn\",columns=\"measType\",values=\"value\")\n",
    "        df5 = df5.rename_axis(\"ID\", axis=1).reset_index()\n",
    "\n",
    "        # Check if other user's counters are in All_scanners. If not put their counters to db only the counters that not in all_scanners\n",
    "        if \"USERDEF-All_Scanners\" not in measInfoId:\n",
    "            blist = list(df5.columns)\n",
    "            blist.remove(\"measObjLdn\")\n",
    "            diff_list = list(set(blist)-set(all_counters))\n",
    "            if len(diff_list)==0:\n",
    "                continue\n",
    "            else:\n",
    "                df5 = df5.drop(list(set(blist) - set(diff_list)),axis=1)\n",
    "                #print(\"Fark listesi: \",diff_list)\n",
    "\n",
    "        df5[\"TimeStamp\"] = TimeStamp\n",
    "        df5[\"SubNetwork1\"] = SubNetwork1\n",
    "        df5[\"SubNetwork2\"] = SubNetwork2\n",
    "        df5[\"SubNetwork3\"] = SubNetwork3\n",
    "        df5[\"MeContext\"] = MeContext\n",
    "\n",
    "\n",
    "        s = df5[\"measObjLdn\"].str.split(\",\")\n",
    "        dfz = pd.DataFrame(s.tolist())\n",
    "\n",
    "        fin_df = pd.DataFrame()\n",
    "        for i in range(len(dfz.columns)):\n",
    "            f = dfz.iloc[:, i].str.split(\"=\")\n",
    "            f = f.dropna()\n",
    "            dfk = pd.DataFrame(np.vstack(f))\n",
    "            dfk.columns = [\"attr\",\"val\"]\n",
    "            dfk[\"ind\"] = range(len(dfk[\"attr\"]))\n",
    "            dfk = dfk.pivot(columns = \"attr\",index=\"ind\",values=\"val\").rename_axis(None, axis=1).reset_index().drop(\"ind\",axis=1)\n",
    "            fin_df = pd.concat([fin_df, dfk], axis=1, join=\"outer\")\n",
    "\n",
    "        df5 = df5.drop(\"measObjLdn\",axis=1)\n",
    "\n",
    "        # Convert the date format for mysql db.\n",
    "        df5 = pd.concat([df5, fin_df], axis=1, join=\"inner\")\n",
    "        df5[\"TimeStamp\"] = df5[\"TimeStamp\"].str.split(\"T\").str[0] + \" \" + df5[\"TimeStamp\"].str.split(\"T\").str[1].str.split(\"+\").str[0]\n",
    "        cols = df5.columns\n",
    "\n",
    "        # Convert columns to numeric if possible.\n",
    "        #for c in cols:\n",
    "        #    try:\n",
    "        #        df5[c] = pd.to_numeric(df5[c])\n",
    "        #    except:\n",
    "        #        pass\n",
    "        #dtyps[measInfoId] = df5.dtypes.to_dict()\n",
    "        a_file = open(\"dtypes_Baseband.pkl\", \"rb\")\n",
    "        dtyps = pickle.load(a_file)\n",
    "        df5 = df5.astype(dtype= dtyps[measInfoId],errors=\"ignore\")\n",
    "            \n",
    "        table_name = measInfoId.replace(\"=\",\"-\")\n",
    "        table_name = table_name.replace(\",\",\"_\")\n",
    "        table_name = table_name.replace(\".\",\"_\")\n",
    "\n",
    "        df5[\"TimeStamp\"] = pd.to_datetime(df5[\"TimeStamp\"])\n",
    "\n",
    "        df5.to_csv(\"csv_baseband/\"+measInfoId,sep=\";\")\n",
    "        # Prepare SQL commands\n",
    "        #with open(\"sql_baseband/\"+measInfoId, \"w\") as f:\n",
    "        with open(\"sql_baseband/sql_commands.txt\", \"a\") as f:\n",
    "            f.write(pd.io.sql.get_schema(df5.reset_index(), measInfoId))\n",
    "        #with open(\"sql_baseband/\"+measInfoId, \"a\") as f:\n",
    "        with open(\"sql_baseband/sql_commands.txt\", \"a\") as f:\n",
    "            f.write(get_insert_query_from_df(df5.reset_index(), measInfoId))\n",
    "    return df5\n",
    "    ## Save\n",
    "    #a_file = open(\"dtypes_Baseband.pkl\", \"wb\")\n",
    "    #pickle.dump(dtyps, a_file)\n",
    "    #a_file.close()\n",
    "\n",
    "def DUS_parser(df_all):\n",
    "    TimeStamp = df_all[\"cbt\"].unique()[0]\n",
    "    SubNetwork1 = df_all[\"sn\"].dropna()[0].split(\",\")[0].split(\"=\")[1]\n",
    "    SubNetwork2 = df_all[\"sn\"].dropna()[0].split(\",\")[1].split(\"=\")[1]\n",
    "    MeContext = df_all[\"sn\"].dropna()[0].split(\",\")[2].split(\"=\")[1]\n",
    "\n",
    "    df_all = df_all[df_all[\"neun\"].isna()]  \n",
    "    df_all = df_all[df_all[\"nedn\"].isna()]\n",
    "    df_all = df_all[df_all[\"nesw\"].isna()]\n",
    "    df_all = df_all[df_all[\"mts\"].isna()]\n",
    "    # Create groups\n",
    "    dfx = df_all[[\"gp\",\"mt\",\"moid\",\"r\",\"sf\"]]\n",
    "\n",
    "    dfx[\"group\"] = range(len(dfx[\"gp\"]))\n",
    "    dfx.loc[dfx[\"gp\"].notna() & dfx[\"mt\"].notna(), 'group'] = None\n",
    "    dfx.loc[dfx[\"gp\"].isna(), 'group'] = None\n",
    "    dfx[\"group\"] = dfx[\"group\"].fillna(method=\"ffill\")\n",
    "\n",
    "    dfx = dfx[dfx[\"sf\"].isna()]\n",
    "\n",
    "    #dtyps = {}\n",
    "    for group in list(dfx.group.dropna().unique()):\n",
    "\n",
    "        dfy = dfx[dfx[\"group\"]==group]\n",
    "        dfy[\"erase\"] = False\n",
    "\n",
    "        dfy = dfy.drop([\"group\",\"erase\"],axis=1)\n",
    "        cols = dfy[\"mt\"].dropna().to_list()\n",
    "        dfy[\"erase\"] = False\n",
    "        dfy.loc[dfy[\"moid\"].notna(), 'erase'] = True\n",
    "        dfy[\"moid\"] = dfy[\"moid\"].fillna(method=\"ffill\")\n",
    "        dfy = dfy[dfy[\"erase\"]==False]\n",
    "        if group==dfx.group.unique()[-1]:\n",
    "            dfy = dfy.iloc[:-1]\n",
    "        dfy = dfy.iloc[:-1]\n",
    "        dfy = dfy[dfy[\"gp\"].isna()]\n",
    "        dfy = dfy[dfy[\"moid\"].notna()]\n",
    "        dfy = dfy.drop([\"mt\",\"erase\",\"gp\"],axis=1)\n",
    "        col_all = []\n",
    "        for i in dfy[\"moid\"].unique():\n",
    "            col_all = col_all + cols\n",
    "        dfy[\"att\"] = col_all    \n",
    "        dfy = dfy.pivot(index=\"moid\",values=\"r\",columns=\"att\").rename_axis(None, axis=1).reset_index()\n",
    "        cols = dfy.columns\n",
    "        dfy[\"TimeStamp\"]=TimeStamp\n",
    "        dfy[\"SubNetwork1\"]=SubNetwork1\n",
    "        dfy[\"SubNetwork2\"]=SubNetwork2\n",
    "        dfy[\"MeContext\"]=MeContext\n",
    "        dfy = dfy[[\"TimeStamp\",\"SubNetwork1\",\"SubNetwork2\",\"MeContext\"] + cols.to_list()]\n",
    "\n",
    "\n",
    "        s = dfy[\"moid\"].str.split(\",\")\n",
    "        dfz = pd.DataFrame(s.tolist())\n",
    "        if len(s)== 0:\n",
    "            continue\n",
    "        fin_df = pd.DataFrame()\n",
    "        for i in range(len(dfz.columns)):\n",
    "            f = dfz.iloc[:, i].str.split(\"=\")\n",
    "            f = f.dropna()\n",
    "            dfk = pd.DataFrame(np.vstack(f))\n",
    "            dfk.columns = [\"attr\",\"val\"]\n",
    "            dfk[\"ind\"] = range(len(dfk[\"attr\"]))\n",
    "            dfk = dfk.pivot(columns = \"attr\",index=\"ind\",values=\"val\").rename_axis(None, axis=1).reset_index().drop(\"ind\",axis=1)\n",
    "            fin_df = pd.concat([fin_df, dfk], axis=1, join=\"outer\")\n",
    "\n",
    "        dfy = dfy.drop(\"moid\",axis=1)\n",
    "        dfy = pd.concat([dfy, fin_df], axis=1, join=\"inner\")\n",
    "\n",
    "        dfy[\"TimeStamp\"] = dfy[\"TimeStamp\"].str[0:4]+\"-\"+dfy[\"TimeStamp\"].str[4:6]+\"-\"+dfy[\"TimeStamp\"].str[6:8]+\" \"+dfy[\"TimeStamp\"].str[8:10]+\":\"+dfy[\"TimeStamp\"].str[10:12]+\":\"+dfy[\"TimeStamp\"].str[12:14]\n",
    "        cols = dfy.columns\n",
    "        \n",
    "        #for c in cols:\n",
    "        #    try:\n",
    "        #        dfy[c] = pd.to_numeric(dfy[c])\n",
    "        #    except:\n",
    "        #        pass\n",
    "        #dtyps[group] = dfy.dtypes.to_dict()\n",
    "        a_file = open(\"dtypes_DUS.pkl\", \"rb\")\n",
    "        dtyps = pickle.load(a_file)\n",
    "        dfy = dfy.astype(dtype= dtyps[group],errors=\"ignore\")\n",
    "        \n",
    "        dfy[\"TimeStamp\"] = pd.to_datetime(dfy[\"TimeStamp\"])\n",
    "\n",
    "        dfy.to_csv(\"csv_DUS/\"+str(group))\n",
    "    ## Save\n",
    "    #a_file = open(\"dtypes_DUS.pkl\", \"wb\")\n",
    "    #pickle.dump(dtyps, a_file)\n",
    "    #a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Run Time:  0:00:04.029616\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.datetime.now()\n",
    "\n",
    "#df_all = pd.read_xml(\"4_problematic.xml\", xpath=\".//*\")\n",
    "#df_all = pd.read_xml(\"4_err.xml\", xpath=\".//*\")\n",
    "df_all = pd.read_xml(\"TEST_DUS/5.xml\", xpath=\".//*\")\n",
    "#df_all = pd.read_xml(\"sample_DUS_PM.xml\", xpath=\".//*\")\n",
    "\n",
    "if \"ffv\" in df_all.columns:\n",
    "    DUS_parser(df_all)\n",
    "elif \"fileFormatVersion\" in df_all.columns:\n",
    "    df = BaseBand_parser(df_all)\n",
    "        \n",
    "t2 = datetime.datetime.now()\n",
    "print(\"Script Run Time: \",t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ifHCInBroadcastPkts   4 non-null      int64         \n",
      " 1   ifHCInMulticastPkts   4 non-null      int64         \n",
      " 2   ifHCInOctets          4 non-null      int64         \n",
      " 3   ifHCInUcastPkts       4 non-null      int64         \n",
      " 4   ifHCOutBroadcastPkts  4 non-null      int64         \n",
      " 5   ifHCOutMulticastPkts  4 non-null      int64         \n",
      " 6   ifHCOutOctets         4 non-null      int64         \n",
      " 7   ifHCOutUcastPkts      4 non-null      int64         \n",
      " 8   ifInDiscards          4 non-null      int64         \n",
      " 9   ifInUnknownProtos     4 non-null      int64         \n",
      " 10  ifOutDiscards         4 non-null      int64         \n",
      " 11  TimeStamp             4 non-null      datetime64[ns]\n",
      " 12  SubNetwork1           4 non-null      object        \n",
      " 13  SubNetwork2           4 non-null      object        \n",
      " 14  SubNetwork3           4 non-null      object        \n",
      " 15  MeContext             4 non-null      object        \n",
      " 16  ManagedElement        4 non-null      object        \n",
      " 17  Transport             4 non-null      int64         \n",
      " 18  VlanPort              4 non-null      object        \n",
      "dtypes: datetime64[ns](1), int64(12), object(6)\n",
      "memory usage: 736.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ifHCInBroadcastPkts   4 non-null      int64         \n",
      " 1   ifHCInMulticastPkts   4 non-null      int64         \n",
      " 2   ifHCInOctets          4 non-null      int64         \n",
      " 3   ifHCInUcastPkts       4 non-null      int64         \n",
      " 4   ifHCOutBroadcastPkts  4 non-null      int64         \n",
      " 5   ifHCOutMulticastPkts  4 non-null      int64         \n",
      " 6   ifHCOutOctets         4 non-null      int64         \n",
      " 7   ifHCOutUcastPkts      4 non-null      int64         \n",
      " 8   ifInDiscards          4 non-null      int64         \n",
      " 9   ifInUnknownProtos     4 non-null      int64         \n",
      " 10  ifOutDiscards         4 non-null      int64         \n",
      " 11  TimeStamp             4 non-null      datetime64[ns]\n",
      " 12  SubNetwork1           4 non-null      object        \n",
      " 13  SubNetwork2           4 non-null      object        \n",
      " 14  SubNetwork3           4 non-null      object        \n",
      " 15  MeContext             4 non-null      object        \n",
      " 16  ManagedElement        4 non-null      object        \n",
      " 17  Transport             4 non-null      int64         \n",
      " 18  VlanPort              4 non-null      object        \n",
      "dtypes: datetime64[ns](1), int64(12), object(6)\n",
      "memory usage: 736.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ifHCInBroadcastPkts   4 non-null      int64         \n",
      " 1   ifHCInMulticastPkts   4 non-null      int64         \n",
      " 2   ifHCInOctets          4 non-null      int64         \n",
      " 3   ifHCInUcastPkts       4 non-null      int64         \n",
      " 4   ifHCOutBroadcastPkts  4 non-null      int64         \n",
      " 5   ifHCOutMulticastPkts  4 non-null      int64         \n",
      " 6   ifHCOutOctets         4 non-null      int64         \n",
      " 7   ifHCOutUcastPkts      4 non-null      int64         \n",
      " 8   ifInDiscards          4 non-null      int64         \n",
      " 9   ifInUnknownProtos     4 non-null      int64         \n",
      " 10  ifOutDiscards         4 non-null      int64         \n",
      " 11  TimeStamp             4 non-null      datetime64[ns]\n",
      " 12  SubNetwork1           4 non-null      object        \n",
      " 13  SubNetwork2           4 non-null      object        \n",
      " 14  SubNetwork3           4 non-null      object        \n",
      " 15  MeContext             4 non-null      object        \n",
      " 16  ManagedElement        4 non-null      object        \n",
      " 17  Transport             4 non-null      int64         \n",
      " 18  VlanPort              4 non-null      object        \n",
      "dtypes: datetime64[ns](1), int64(12), object(6)\n",
      "memory usage: 736.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ifHCInBroadcastPkts   3 non-null      int64         \n",
      " 1   ifHCInMulticastPkts   3 non-null      int64         \n",
      " 2   ifHCInOctets          3 non-null      int64         \n",
      " 3   ifHCInUcastPkts       3 non-null      int64         \n",
      " 4   ifHCOutBroadcastPkts  3 non-null      int64         \n",
      " 5   ifHCOutMulticastPkts  3 non-null      int64         \n",
      " 6   ifHCOutOctets         3 non-null      int64         \n",
      " 7   ifHCOutUcastPkts      3 non-null      int64         \n",
      " 8   ifInDiscards          3 non-null      int64         \n",
      " 9   ifInUnknownProtos     3 non-null      int64         \n",
      " 10  ifOutDiscards         3 non-null      int64         \n",
      " 11  TimeStamp             3 non-null      datetime64[ns]\n",
      " 12  SubNetwork1           3 non-null      object        \n",
      " 13  SubNetwork2           3 non-null      object        \n",
      " 14  SubNetwork3           3 non-null      object        \n",
      " 15  MeContext             3 non-null      object        \n",
      " 16  ManagedElement        3 non-null      object        \n",
      " 17  Transport             3 non-null      int64         \n",
      " 18  VlanPort              3 non-null      object        \n",
      "dtypes: datetime64[ns](1), int64(12), object(6)\n",
      "memory usage: 584.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
